# -*- coding: utf-8 -*-
"""TTMWorking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z5YprdjiJy6KXUIsjGrsViu_x6AAMbTc

# 1 Recommender Model

Importing libraries
"""

import tensorflow as tf
from tensorflow.keras import layers, Model
import pandas as pd
import numpy as np
import ast
from keras.optimizers import AdamW
import random
tf.random.set_seed(42)
np.random.seed(42)
random.seed(42)

tf.__version__

"""Importing training dataset and pre-processing it"""

# Load Excel files into pandas DataFrames
# creatives_df = pd.read_csv('/content/llb_merged_output.csv')
creatives_df = pd.read_csv('/content/swarm_roas_final.csv')
audience_df = pd.read_csv('/content/swarm_roas_final.csv')
ctr_df = pd.read_csv('/content/swarm_roas_final.csv')


creatives_df['Creative_embeddings'] = creatives_df['Creative_embeddings'].fillna('[]')
# creatives_df['Creatives_embedding'] = creatives_df['Averaged_Embedding'].fillna('[]')  # Replace 'nan' with empty list

#creatives_df['Creatives_embedding'] = creatives_df['Embedding'].fillna('[]')  # Replace 'nan' with empty list
# Extract and process the relevant columns and Pad the embeddings to ensure they have the same length
max_length = max(len(eval(embed)) for embed in creatives_df['Creative_embeddings'])
X_creatives = np.array([eval(embed) + [0] * (max_length - len(eval(embed))) for embed in creatives_df['Creative_embeddings']])# The embeddings are padded with zeros to make them the same length



# Convert the Audience embedding column to a list of lists
def parse_embedding(embedding_str):
    # Remove brackets, split the string, and handle newlines
    embedding_str = embedding_str.strip('[]')  # Remove brackets
    components = embedding_str.replace('\n', '').split() # Split by whitespace to separate numbers
    # Convert the components to floats, handling potential errors
    return [float(comp) for comp in components if comp.strip()] # Ignore empty strings

X_audiences_list = [parse_embedding(embed) for embed in audience_df['Summed Embedding']]

# Find the maximum length of audience embeddings
max_audience_length = max(len(embed) for embed in X_audiences_list)

# Pad audience embeddings to have the same length
X_audiences_padded = [list(embed) + [0] * (max_audience_length - len(embed)) for embed in X_audiences_list]
# Now convert the list of lists to a numpy array
X_audiences = np.array(X_audiences_padded)  # Use the padded list


# x_ctr=ctr_df['label']
# Extract the CTR values as a numpy array
y_ctr = ctr_df['label'].to_numpy()
# X_creatives_combined = np.concatenate([X_creatives, X_objectives, X_goal], axis=1)

# X_combined now contains the concatenated embeddings of creatives, objectives, and goals
# print(X_creatives_combined.shape)

np.save('creatives_embeddings.npy', X_creatives)
np.save('audience_embeddings.npy', X_audiences)
np.save('ctr_values.npy', y_ctr)
print(X_creatives.shape)
print("Shape of X_audiences:", X_audiences.shape)
print("Shape of y_ctr:", y_ctr.shape)






# # Convert and pad objective embeddings
# max_objective_length = max(len(eval(embed)) for embed in creatives_df['objective_embedding'])
# X_objectives1 = np.array([eval(embed) + [0] * (max_objective_length - len(eval(embed))) for embed in creatives_df['objective_embedding']])
# X_objectives=layers.Dense(128, activation=None)(X_objectives1)


# # Convert and pad objective embeddings
# max_goal_length = max(len(eval(embed)) for embed in creatives_df['goal_embedding'])
# X_goal1 = np.array([eval(embed) + [0] * (max_objective_length - len(eval(embed))) for embed in creatives_df['goal_embedding']])
# X_goal=layers.Dense(128, activation=None)(X_goal1)
# X_creatives_combined = np.concatenate([X_creatives, X_objectives, X_goal], axis=1)

# X_combined now contains the concatenated embeddings of creatives, objectives, and goals
# print(X_creatives_combined.shape)

print(X_creatives[0])

"""Two-Tower Model Implementation"""

import random
tf.random.set_seed(42)
np.random.seed(42)
random.seed(42)

# Step 1: Data Preparation
# creatives_embeddings_dim=664
creatives_embeddings_dim=1024 #1408 for google multimodal embeddings
audience_embeddings_dim=768



# Step 2: Define Two-Tower Model
def create_creative_tower(embeddings_dim):
    creative_input = layers.Input(shape=(embeddings_dim,), name='creative_input')
    # x = layers.Dense(1408, activation='relu')(creative_input)
    x = layers.Dense(704, activation='relu')(creative_input)
    x = layers.Dense(216, activation='relu')(x)
    creative_output = layers.Dense(128, activation=None)(x) # Change output dimension to 32
    return Model(inputs=creative_input, outputs=creative_output, name='creative_tower')

def create_audience_tower(embeddings_dim):
    audience_input = layers.Input(shape=(embeddings_dim,), name='audience_input') #
    y = layers.Dense(532, activation='relu')(audience_input)
    y = layers.Dense(256, activation='relu')(y)
    audience_output = layers.Dense(128, activation=None)(y)
    return Model(inputs=audience_input, outputs=audience_output, name='audience_tower')

# Instantiate towers
creative_tower = create_creative_tower(creatives_embeddings_dim)
audience_tower = create_audience_tower(audience_embeddings_dim) # No need to pass embeddings_dim here as it's hardcoded in the function now


# Combine towers
creative_output = creative_tower.output
audience_output = audience_tower.output
similarity = layers.Dot(axes=-1, normalize=True)([creative_output, audience_output])
model = Model(inputs=[creative_tower.input, audience_tower.input], outputs=similarity)

"""Training"""

# Set random seed for reproducibility
import random
tf.random.set_seed(42)
np.random.seed(42)
random.seed(42)



# Compile the model with binary cross-entropy loss since we have binary labels for CTR
model.compile(optimizer=AdamW(), loss='binary_crossentropy', metrics=['accuracy'])  # Changed loss function to binary cross-entropy
# # Compile the model with mean squared error loss for regression
# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])



# Import ModelCheckpoint callback
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint to save the model with the lowest validation loss
checkpoint = ModelCheckpoint(
    'best_model.keras',  # Name of the file to save the model to
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)


# Train the model
from sklearn.model_selection import train_test_split


# Split data
X_creative_train, X_creative_val, X_audience_train, X_audience_val, y_train, y_val = train_test_split(
    X_creatives, X_audiences, y_ctr, test_size=0.2, random_state=42
)


# Training the model with the checkpoint callback
history = model.fit(
    [X_creative_train, X_audience_train], y_train,
    validation_data=([X_creative_val, X_audience_val], y_val),
    epochs=40,
    batch_size=16,
    callbacks=[checkpoint]  # Add the checkpoint callback here
)


# Extract trained embeddings after training
creative_embeddings = creative_tower.predict(X_creatives)
audience_embeddings = audience_tower.predict(X_audiences)

# Load the saved model
best_model = tf.keras.models.load_model('best_model.keras')

# Evaluate the loaded model on the validation set
val_loss, val_accuracy = best_model.evaluate([X_creative_val, X_audience_val], y_val)

print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")



"""# Loading Interest Universe"""

# Load interest embeddings
interest_embeddings_df = pd.read_excel('/content/final_concatenated_universe_interests_converted_embed.xlsx')

"""Audience data preprocessing"""

import re

# Function to clean and convert space-separated strings to lists of floats
def convert_and_clean_embedding(embedding_str):
    # Remove any extraneous characters like brackets
    cleaned_str = re.sub(r'[\[\]\']', '', embedding_str)
    # Convert cleaned string to list of floats, splitting on commas
    return list(map(float, cleaned_str.split(',')))  # Split on commas here

# Apply the conversion function
interest_embeddings_df['embedding'] = interest_embeddings_df['embedding'].apply(convert_and_clean_embedding)

# Find the maximum length of embeddings
max_length = max(len(embed) for embed in interest_embeddings_df['embedding'])

# Pad embeddings to ensure all have the same length
padded_embeddings = [embed + [0] * (max_length - len(embed)) for embed in interest_embeddings_df['embedding']]

# Convert to numpy array
interest_embeddings = np.array(padded_embeddings)
np.save('interest_embeddings.npy', interest_embeddings)
# Example of how to use the embeddings
print(interest_embeddings[:5])
print(interest_embeddings.shape)

"""Passing the audience dataset through audience tower"""

interest_names = interest_embeddings_df['name'].tolist()

# Convert to numpy array
X_all_interests = np.array(interest_embeddings)

# Load the saved model
best_model = tf.keras.models.load_model('best_model.keras', custom_objects={'ModelCheckpoint': ModelCheckpoint})

interest_names = interest_embeddings_df['name'].tolist()
# Convert to numpy array
X_all_interests = np.array(interest_embeddings)

# Pass the reshaped embeddings through the audience tower
audience_embeddings_transformed = audience_tower.predict(X_all_interests)
print(audience_embeddings_transformed.shape)
audience_embeddings_transformed[26]

creative_embed_tran= creative_tower.predict(X_creatives)

print(audience_embeddings_transformed[:5])

print(creative_embed_tran[:5]) # print the first 5 rows of the array

import pandas as pd
import json

# Function to clean up the JSON string
def clean_json_string(json_str):
    try:
        # Replace escaped backslashes and double quotes
        clean_str = json_str.replace('\\"', '"').replace("\\", "")
        return json.loads(clean_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        return []

# Function to convert targeting spec into English statement
def targeting_to_english(targeting):
    statement = []

    for spec in targeting:
        parts = []
        for key, value in spec.items():
            if key in ['behaviors', 'interests', 'life_events']:
                item_names = [f"{item['name']} (id: {item['id']})" for item in value]
                parts.append(f"({' OR '.join(item_names)})")
        if parts:
            statement.append(" AND ".join(parts))

    return " AND ".join(statement)

# Load the CSV file
file_path = '/content/bluf_pre.csv'
df = pd.read_csv(file_path)

english_statements = []

# Process the targeting columns and generate English statements
for index, row in df.iterrows():
    targeting_spec = clean_json_string(row['includedInterests']) if pd.notna(row['includedInterests']) else []

    # Generate the English statement
    english_statement = targeting_to_english(targeting_spec)

    # Append the statement to the list
    english_statements.append(english_statement)

# Add the English statements to the DataFrame as a new column
df['Targeting English Statement'] = english_statements

# Save the DataFrame to a new Excel file
output_file_path = '/content/includedbehavior_targeting_ad_sets_with_statements.xlsx'
df.to_excel(output_file_path, index=False)

print(f"Processed file saved to: {output_file_path}")
df['Targeting English Statement'].head(50)

import pandas as pd
import numpy as np
import json

import re
from transformers import BertModel, BertTokenizer
import torch

# Load BERT model and tokenizer for computing behavior embeddings
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Load the Excel file containing interest embeddings
embedding_file_path = '/content/final_concatenated_universe_interests_converted_embed.xlsx'
df_embeddings = pd.read_excel(embedding_file_path)
embedding_dict = {str(row['id']): np.array(list(map(float, row['embedding'].split(',')))) for _, row in df_embeddings.iterrows()}

# Load the main data with the JSON values
data_path = '/content/includedbehavior_targeting_ad_sets_with_statements.xlsx'
df_statements = pd.read_excel(data_path)

# Function to clean up the JSON string
def clean_json_string(json_str):
    try:
        # Replace escaped backslashes and double quotes
        clean_str = json_str.replace('\\"', '"').replace("\\", "")
        return json.loads(clean_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        return []

# Function to convert targeting spec into AND/OR English statement
def targeting_to_english(targeting):
    statement = []
    for spec in targeting:
        parts = []
        for key, value in spec.items():
            if key in ['behaviors', 'interests', 'life_events']:
                item_names = [f"{item['name']} (id: {item['id']})" for item in value]
                parts.append(f"({' OR '.join(item_names)})")
        if parts:
            statement.append(" AND ".join(parts))
    return " AND ".join(statement)

# Function to get embedding from the dictionary
def get_embedding(word_id, embedding_dict):
    return embedding_dict.get(str(word_id), np.ones(768))  # Return ones for AND operations

# Parse JSON, generate English statements, and calculate embeddings with AND/OR operations
def parse_and_compute_embeddings(row):
    # Clean and parse JSON
    json_data = clean_json_string(row['includedInterests'])
    if not json_data:
        return np.zeros(768)  # Return zero vector if JSON is invalid

    # Generate English statement
    english_statement = targeting_to_english(json_data)

    # Initialize lists for embeddings
    behavior_embeddings = []
    interest_embeddings = []

    # Process behaviors: Compute BERT embeddings for each behavior name
    if 'behaviors' in json_data[0]:
        for behavior in json_data[0]['behaviors']:
            name = behavior['name']
            inputs = tokenizer(name, return_tensors='pt')
            with torch.no_grad():
                outputs = model(**inputs)
            behavior_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling
            behavior_embeddings.append(behavior_embedding)

    # Process interests: Retrieve embeddings from Excel using interest IDs
    if 'interests' in json_data[0]:
        for interest in json_data[0]['interests']:
            interest_id = interest['id']
            interest_embedding = get_embedding(interest_id, embedding_dict)
            interest_embeddings.append(interest_embedding)

    # Initialize result embeddings for AND/OR logic
    combined_embedding = np.ones(768)  # Start with ones for AND logic
    for part in english_statement.split(" AND "):
        # Initialize temporary OR embedding within each AND group
        or_embedding = np.zeros(768)  # Start with zero for OR logic

        for subpart in part.split(" OR "):
            if "id:" in subpart:
                id_matches = re.findall(r'id:\s*(\d+)', subpart)
                if id_matches:
                    word_id = int(id_matches[0])
                    embedding = get_embedding(word_id, embedding_dict)

                    # OR logic: Add embeddings within each group
                    or_embedding += embedding

        # AND logic: Multiply results of OR groups
        combined_embedding *= or_embedding  # Multiply each AND group

    # Combine embeddings: Sum behavior and interest embeddings with AND/OR embedding results
    final_embedding = np.sum(behavior_embeddings + interest_embeddings, axis=0) + combined_embedding
    return final_embedding, english_statement

# Apply the function to each row to get combined embeddings and English statements
results = df_statements.apply(parse_and_compute_embeddings, axis=1)
df_statements['Summed Embedding'], df_statements['Targeting English Statement'] = zip(*results)

# Save the updated table to a new CSV file
output_path = '/content/final_bluffbet_pre_data_with_embeddings.csv'
df_statements.to_csv(output_path, index=False)
print(f"Updated statement table saved as {output_path}")

# Display first 5 rows for verification
print(df_statements[['Targeting English Statement', 'Summed Embedding']].head())

"""# 1. Predictions using Scann"""

import pandas as pd
import numpy as np

# Load the Excel sheet containing the embeddings
df = pd.read_excel("/content/final_concatenated_universe_interests_converted_embed.xlsx")  # Replace with the actual path to your file

# List of new interest IDs from your updated predictions
top_20_interest_ids = [
    "6002950344574", "6003011442797", "6003012317397", "6003026931693", "6003030519207",
    "6003108445833", "6003114768317", "6003126253149", "6003153435265", "6003157054100",
    "6003237046675", "6003248338072", "6003359999423", "6003394230131", "6003484127669",
    "6003667083304", "6003736288753", "6009608672113"
]


# Ensure both 'id' column and interest IDs are of the same data type (string)
df['id'] = df['id'].astype(str)
top_20_interest_ids = [str(x) for x in top_20_interest_ids]

# Filter the DataFrame for rows where the 'id' column matches one of the top 20 interest IDs
filtered_df = df[df['id'].isin(top_20_interest_ids)]

# Check if filtered_df is empty and provide feedback
if filtered_df.empty:
    print("No matching interest IDs found in the DataFrame. Please check your 'top_20_interest_ids' list.")
else:
    # Convert the 'embedding' column from strings to numpy arrays
    filtered_df['Summed Embedding'] = filtered_df['embedding'].apply(lambda x: np.fromstring(x, sep=','))

    # Stack all embeddings and compute the average
    total_embedding = np.mean(np.stack(filtered_df['Summed Embedding'].values), axis=0)

    # Print the result
    print("total_embedding  of Top 50 Interests:", total_embedding )

!pip install scann

# Load interest embeddings
creat_embeddings_df = pd.read_excel('/content/LLB_output_embeddings.xlsx')
max_length = max(len(eval(embed)) for embed in creat_embeddings_df['Averaged_Embedding'])
X_test_creatives = np.array([eval(embed) + [0] * (max_length - len(eval(embed))) for embed in creat_embeddings_df['Averaged_Embedding']])


# X_test_creatives_combined = np.concatenate([X_creatives, X_objectives, X_goal], axis=1)

"""negative"""

import scann
# Example: Using SCANN with the original dot product and then inverting the results
searcher = scann.scann_ops_pybind.builder(audience_embeddings_transformed, 20, "dot_product") \
    .tree(num_leaves=168, num_leaves_to_search=100, training_sample_size=155) \
    .score_ah(2, anisotropic_quantization_threshold=0.2) \
    .reorder(100) \
    .build()
creative_embedding = X_creatives[1]
# Reshape to add a batch dimension AND specify the shape
# creative_embedding = np.expand_dims(creative_embedding, axis=0).astype(np.float32)  # Assuming your model expects float32
creative_embedding_transformed = creative_tower.predict(creative_embedding)

# Search for the top 10 most similar interest embeddings
indices, distances = searcher.search(creative_embedding_transformed[0])


# Invert the distances for negative recommendations
inverted_distances = -distances  # Negate the distances to reflect the "avoid" recommendations

# Sort indices by inverted distances (higher negative value indicates a stronger recommendation to avoid)
sorted_indices = np.argsort(inverted_distances)

# Output the top indices and their inverted distances
print("Top 20 audiences to avoid for creative {}: (Inverted Recommendations)")
for idx in sorted_indices[:20]:  # Get top 10 based on the inverted distance
    print(f"Interest Name: {interest_names[idx]}, Inverted Similarity Score: {inverted_distances[idx]}")

import scann
# Get the list of interest IDs
interest_ids = interest_embeddings_df['id'].tolist()

# Define the SCANN searcher for the audience embeddings
searcher = scann.scann_ops_pybind.builder(audience_embeddings_transformed, 50, "dot_product") \
    .tree(num_leaves=168, num_leaves_to_search=100, training_sample_size=200).score_ah(2, anisotropic_quantization_threshold=0.2) \
    .reorder(100) \
    .build()

# Prepare the creative embedding
creative_embedding = X_creatives[4]
creative_embedding = np.expand_dims(creative_embedding, axis=0).astype(np.float32)  # Assuming your model expects float32
creative_embedding_transformed = creative_tower.predict(creative_embedding)

# Search for the top 20 most similar interest embeddings
indices, distances = searcher.search(creative_embedding_transformed[0])

# Output the top indices, distances, interest names, and corresponding ids
print("Top 50 most similar interests for the given creative:")
for idx, distance in zip(indices, distances):
    interest_name = interest_names[idx]
    # Retrieve the interest ID using the index from the interest_ids list
    interest_id = interest_ids[idx]
    print(f"Interest ID: {interest_id}, Interest Name: {interest_name}, Distance: {distance}")

"""top10 search, hit ratio-evaluation,ctr-prediction, confidence on how it will hit

#log model into model registry
"""

!pip install -U "truefoundry"
!pip install -U "truefoundry[ml]"
!pip install -U "truefoundry[workflow]"

import tensorflow as tf
from tensorflow.keras import layers, Model
import pandas as pd
import numpy as np
import ast
from tensorflow.keras.optimizers import AdamW
import random
from truefoundry.ml import get_client, ModelFramework

!tfy login --host https://storyteller.truefoundry.cloud

import tensorflow as tf
from tensorflow.keras import layers, Model
import pandas as pd
import numpy as np
import ast
from tensorflow.keras.optimizers import AdamW
import random
from truefoundry.ml import get_client, ModelFramework

import tensorflow as tf

# Save the models using the SavedModel format
tf.saved_model.save(creative_tower, "creative_tower_model")  # This will create a directory named 'creative_tower_model' with the .pb file inside
tf.saved_model.save(audience_tower, "audience_tower_model")  # This will create a directory named 'audience_tower_model' with the .pb file inside

# Use TrueFoundry client to log models
client = get_client()


# Ensure the repo exists by explicitly checking for its existence before logging the model
try:
    client.get_ml_repo(name="two-tower-model")
    print(f"ML repo: two-tower-model exists")
except Exception as e:
    print(f"Error: ML repo two-tower-model does not exist or there was an error accessing it: {e}")
    # Handle the error, e.g., by creating the repository
    try:
        client.create_ml_repo(ml_repo="two-tower-model", storage_integration_fqn=None)
        print(f"Created ML repo: two-tower-model")
    except Exception as e:
        print(f"Error creating ML repo: {e}")

# Log creative tower model
creative_model_version = client.log_model(
    ml_repo="two-tower-model",
    name="creative-tower",
    model_file_or_folder="creative_tower_model",
    framework=ModelFramework.TENSORFLOW,
    metadata={"description": "Creative tower model of the two-tower system"},

)

print(f"Creative Tower Model logged with FQN: {creative_model_version.fqn}")

audience_model_version = client.log_model(
    ml_repo="two-tower-model",
    name="audience-tower",
    model_file_or_folder="audience_tower_model",
    framework=ModelFramework.TENSORFLOW,
    metadata={"description": "Audience tower model of the two-tower system"},

)

print(f"Audience Tower Model logged with FQN: {audience_model_version.fqn}")

import os
import numpy as np
import tempfile
import tensorflow as tf
from truefoundry.ml import get_client
from tensorflow.keras.layers import TFSMLayer # Import the TFSMLayer

client = get_client()
model_version_creative = client.get_model_version_by_fqn(
    fqn="model:storyteller/two-tower-model/11_33_creative_tower:1"
)
model_version_audience = client.get_model_version_by_fqn(
    fqn="model:storyteller/two-tower-model/11_33_audience_tower:1"
)


# Step 2: Create separate temporary directories and Download the Models to Disk
temp_creative = tempfile.TemporaryDirectory()
temp_audience = tempfile.TemporaryDirectory()

download_info_creative = model_version_creative.download(path=temp_creative.name)
download_info_audience = model_version_audience.download(path=temp_audience.name)

# Print directory structure for debugging
print("Creative Tower Directory Structure:")
for root, dirs, files in os.walk(download_info_creative.model_dir):
    level = root.replace(download_info_creative.model_dir, '').count(os.sep)
    indent = ' ' * 4 * (level)
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files:
        print(f"{subindent}{f}")

print("Audience Tower Directory Structure:")
for root, dirs, files in os.walk(download_info_audience.model_dir):
    level = root.replace(download_info_audience.model_dir, '').count(os.sep)
    indent = ' ' * 4 * (level)
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files:
        print(f"{subindent}{f}")

# Step 3: Update the Path to the SavedModel Based on the Correct Directory Structure
# After examining the directory structure, adjust the path as necessary.
# Example (you may need to modify this based on the actual output):
creative_model_path = os.path.join(download_info_creative.model_dir)  # Adjust if necessary
audience_model_path = os.path.join(download_info_audience.model_dir)  # Adjust if necessary

# Load the models
creative_tower = TFSMLayer(creative_model_path, call_endpoint='serving_default') # Use TFSMLayer to load the model
audience_tower = TFSMLayer(audience_model_path, call_endpoint='serving_default') # Use TFSMLayer to load the model

# Step 4: Prepare the Input Data for Prediction
# Assuming X_creatives and X_audiences are already loaded or defined as numpy arrays
# For demonstration purposes, using a single sample from each to predict similarity

sample_creative_embedding = np.expand_dims(X_creatives[0], axis=0).astype(np.float32)
sample_audience_embedding = np.expand_dims(X_audiences[0], axis=0).astype(np.float32)

# Step 5: Generate Predictions
creative_embedding_transformed = creative_tower(sample_creative_embedding) # Use the loaded layer for prediction
audience_embedding_transformed = audience_tower(sample_audience_embedding) # Use the loaded layer for prediction

# Calculate similarity using Dot Product (the same way as in the combined model)
similarity_layer = tf.keras.layers.Dot(axes=-1, normalize=True)
similarity_score = similarity_layer([creative_embedding_transformed, audience_embedding_transformed])

print(f"Predicted Similarity Score: {similarity_score.numpy()[0][0]}")

# Clean up the temporary directories (optional, as they are automatically cleaned when deleted)
temp_creative.cleanup()
temp_audience.cleanup()

"""#Downloading models from truefoundry"""

from truefoundry.ml import get_client
import os # import the os module

client = get_client()

# Get the model version directly
model_version = client.get_model_version_by_fqn("model:storyteller/two-tower-model/6696ea6e6fd581b7fb9a800d_6594260f7da4dd34af9707e6_audience_tower:7")

# Create the directory if it doesn't exist
if not os.path.exists("model_downloads2"):
    os.makedirs("model_downloads2")

# Download it to disk
# `download_info.model_dir` points to a directory which has contents of the model
download_info = model_version.download(path="model_downloads2")

model_version = client.get_model_version_by_fqn("model:storyteller/two-tower-model/6696ea6e6fd581b7fb9a800d_6594260f7da4dd34af9707e6_creative_tower:8")

# Create the directory if it doesn't exist
if not os.path.exists("model_downloads1"):
    os.makedirs("model_downloads1")

# Download it to disk
# `download_info.model_dir` points to a directory which has contents of the model
download_info = model_version.download(path="model_downloads1")

audience_model_path = "model_downloads2/files/model"



audience_tower = TFSMLayer(audience_model_path, call_endpoint='serving_default') # Load using TFSMLayer


sample_audience_embeddings = X_all_interests.astype(np.float32)
audience_embeddings_transformed = audience_tower(X_all_interests)

!pip install scann # Install scann
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import TFSMLayer # Import the TFSMLayer
import scann # Import the scann module

# Correct paths after verifying the directory structure
audience_model_path = "model_downloads2/files/model"



audience_tower = TFSMLayer(audience_model_path, call_endpoint='serving_default') # Load using TFSMLayer


sample_audience_embeddings = X_all_interests.astype(np.float32)  # Ensure float32 dtype

audience_embeddings_transformed = audience_tower(X_all_interests)  # Get transformed audience embeddings

audience_embedding_array = audience_embeddings_transformed['output_0']
print("Audience Embedding Sample:\n", audience_embedding_array[:5])

print("Audience Embedding Sample:\n", audience_embedding_array[:5])

# Correct paths after verifying the directory structure
creative_model_path = "model_downloads1/files/model"


# Step 1: Load the Saved Models using TFSMLayer
creative_tower = TFSMLayer(creative_model_path, call_endpoint='serving_default') # Load using TFSMLayer


sample_creative_embeddings = X_creatives.astype(np.float32)  # Ensure float32 dtype


creative_embeddings_transformed = creative_tower(X_creatives)  # Get transformed creative embeddings



# creative_embedding_array = creative_embeddings_transformed['output_0']


# print("Creative Embedding Shape:", creative_embedding_array.shape)

creative_embedding_array = creative_embeddings_transformed['output_0']

print("Creative Embedding Sample:\n", creative_embedding_array[:5])

print(X_creatives[0])

import scann

# Extract the embedding array from the dictionary
audience_embedding_array = audience_embeddings_transformed['output_0']

# Now use the embedding array to build the searcher
searcher = scann.scann_ops_pybind.builder(audience_embedding_array, 10, "dot_product") \
    .tree(num_leaves=168, num_leaves_to_search=100, training_sample_size=155).score_ah(2, anisotropic_quantization_threshold=0.2) \
    .reorder(100) \
    .build()

# creative_embedding = np.concatenate([X_test_creatives[0], X_objectives[0], X_goal[0]])
creative_embedding = X_creatives[21]
# Reshape to add a batch dimension AND specify the shape
creative_embedding = np.expand_dims(creative_embedding, axis=0).astype(np.float32)  # Assuming your model expects float32
# Call the TFSMLayer directly like a function for inference
creative_embedding_transformed = creative_tower(creative_embedding)

# Access the output tensor using the correct key (e.g., 'output_0')
creative_embedding_to_search = creative_embedding_transformed['output_0']  # Replace 'output_0' with the actual output key if different

# Reshape the creative_embedding_to_search to be 1-dimensional
# Use tf.reshape to reshape the EagerTensor
creative_embedding_to_search = tf.reshape(creative_embedding_to_search, [-1])  # Flatten the array using tf.reshape

# Search for the top 10 most similar interest embeddings
indices, distances = searcher.search(creative_embedding_to_search)  # Use the extracted embedding for search
# inverted_distances = -distances
# Output the top indices, their distances, and corresponding interest names
print("Top 10 most similar interests for the given creative:image3")
for idx, distance in zip(indices, distances):
    print(f"Interest Name: {interest_names[idx]}, Distance: {distance}")

import scann

# Extract the embedding array from the dictionary
audience_embedding_array = audience_embeddings_transformed['output_0']

# Now use the embedding array to build the searcher
searcher = scann.scann_ops_pybind.builder(audience_embedding_array, 10, "dot_product") \
    .tree(num_leaves=168, num_leaves_to_search=100, training_sample_size=155).score_ah(2, anisotropic_quantization_threshold=0.2) \
    .reorder(100) \
    .build()

# creative_embedding = np.concatenate([X_test_creatives[0], X_objectives[0], X_goal[0]])
creative_embedding = X_creatives[0]
# Reshape to add a batch dimension AND specify the shape
creative_embedding = np.expand_dims(creative_embedding, axis=0).astype(np.float32)  # Assuming your model expects float32
# Call the TFSMLayer directly like a function for inference
creative_embedding_transformed = creative_tower(creative_embedding)

# Access the output tensor using the correct key (e.g., 'output_0')
creative_embedding_to_search = creative_embedding_transformed['output_0']  # Replace 'output_0' with the actual output key if different

# Reshape the creative_embedding_to_search to be 1-dimensional
# Use tf.reshape to reshape the EagerTensor
creative_embedding_to_search = tf.reshape(creative_embedding_to_search, [-1])  # Flatten the array using tf.reshape

# Search for the top 10 most similar interest embeddings
indices, distances = searcher.search(creative_embedding_to_search)  # Use the extracted embedding for search
inverted_distances = -distances
# Output the top indices, their distances, and corresponding interest names
print("Top 10 most similar interests for the given creative:image3")
for idx, distance in zip(indices, distances):
    print(f"Interest Name: {interest_names[idx]}, Distance: {distance}")

